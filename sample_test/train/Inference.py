import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ================= é…ç½®è·¯å¾„ =================
# 1. åŸºåº§æ¨¡å‹è·¯å¾„ (ä½ çš„æœ¬åœ°è·¯å¾„)
BASE_MODEL_PATH = "/home/mac/PycharmProjects/PythonProject/yoyo/models/Qwen2.5-Coder-7B-Instruct"

# 2. åˆšåˆšè®­ç»ƒå¥½çš„ LoRA æƒé‡è·¯å¾„
LORA_PATH = "/home/mac/PycharmProjects/PythonProject/yoyo/solhint/lora/solidity_lintseq"

# ===========================================

def main():
    print("ğŸš€ æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹...")
    # 1. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    
    # 2. åŠ è½½åŸºåº§æ¨¡å‹ (ä½¿ç”¨ BF16 å’Œ SDPA åŠ é€Ÿ)
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation="sdpa", # æ¨ç†æ—¶ä¹Ÿå¯ä»¥ç”¨ sdpa åŠ é€Ÿ
        trust_remote_code=True
    )

    print(f"ğŸ”— æ­£åœ¨æŒ‚è½½ LoRA æƒé‡: {LORA_PATH} ...")
    # 3. åŠ è½½å¹¶åˆå¹¶ LoRA æƒé‡
    # è¿™æ­¥æ“ä½œä¸ä¼šä¿®æ”¹ç¡¬ç›˜ä¸Šçš„æ–‡ä»¶ï¼Œåªæ˜¯åœ¨æ˜¾å­˜é‡ŒæŠŠ LoRA è´´ä¸Šå»
    model = PeftModel.from_pretrained(base_model, LORA_PATH)
    
    # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼Œå‡†å¤‡ç”Ÿæˆï¼")

    # ================= æµ‹è¯•æ¡ˆä¾‹ =================
    
    # è¿™é‡Œå†™ä¸€ä¸ªä½ æƒ³æµ‹è¯•çš„ Prompt
    # æ³¨æ„ï¼šè¿™é‡Œçš„ Instruction é£æ ¼è¦å’Œä½ è®­ç»ƒé›†é‡Œçš„ä¿æŒä¸€è‡´
    instruction = "Create a standard ERC20 token contract named 'MyToken' with symbol 'MTK'."
    input_text = "" # å¦‚æœæœ‰ input å°±å¡«ï¼Œæ²¡æœ‰ç•™ç©º

    # 4. æ„é€ å¯¹è¯æ ¼å¼ (ChatML)
    if input_text:
        content = f"{instruction}\n\nInput:\n{input_text}"
    else:
        content = instruction

    messages = [
        {"role": "user", "content": content}
    ]
    
    # åº”ç”¨æ¨¡æ¿
    text = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼Œå‘Šè¯‰æ¨¡å‹è¯¥è½®åˆ° assistant è¯´è¯äº†
    )

    # 5. ç¼–ç è¾“å…¥
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # 6. ç”Ÿæˆä»£ç 
    print("\nğŸ¤– æ­£åœ¨ç”Ÿæˆå›å¤...\n" + "="*50)
    
    with torch.no_grad():
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=1024,   # ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
            temperature=0.2,       # æ¸©åº¦ä½ä¸€ç‚¹ï¼Œä»£ç ç”Ÿæˆçš„é€»è¾‘æ›´ä¸¥è°¨
            top_p=0.9,
            do_sample=True
        )

    # 7. è§£ç è¾“å‡º (å»æ‰è¾“å…¥çš„ Prompt éƒ¨åˆ†ï¼Œåªçœ‹æ–°ç”Ÿæˆçš„)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print(response)
    print("="*50)

if __name__ == "__main__":
    main()
