import torch
import json
import re
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datasets import load_dataset

# ================= é…ç½®è·¯å¾„ =================
BASE_MODEL = "/home/mac/PycharmProjects/PythonProject/yoyo/models/Qwen2.5-Coder-7B-Instruct"
LORA_PATH = "/home/mac/PycharmProjects/PythonProject/yoyo/solhint/lora/solidity_lintseq"
OUTPUT_FILE = "solutions.json"

def clean_lintseq_code(text):
    """
    æ¸…æ´—å‡½æ•°ï¼šæŠŠæ¨¡å‹ç”Ÿæˆçš„ Diff æ ¼å¼è¿˜åŸæˆæ ‡å‡† Solidity ä»£ç 
    """
    lines = text.split('\n')
    code_lines = []
    for line in lines:
        # å»æ‰ diff å¤´éƒ¨ä¿¡æ¯
        if line.startswith('@@') and line.endswith('@@'):
            continue
        # å»æ‰åˆ é™¤çº¿
        if line.startswith('-'):
            continue
        # æå–æ–°å¢çº¿ (å»æ‰ + å·)
        if line.startswith('+'):
            code_lines.append(line[1:]) 
        else:
            # ä¿ç•™åŸæœ¬æ²¡æœ‰æ ‡è®°çš„è¡Œ
            code_lines.append(line)
    return '\n'.join(code_lines)

def main():
    # 1. åŠ è½½æ¨¡å‹
    print("ğŸš€ Loading Model...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL, 
        torch_dtype=torch.bfloat16, 
        attn_implementation="sdpa", 
        device_map="auto",
        trust_remote_code=True
    )
    model = PeftModel.from_pretrained(base_model, LORA_PATH)
    model.eval()

    # 2. åŠ è½½é¢˜ç›®é›†
    print("Loading HumanEval-Solidity...")
    dataset = load_dataset("structures-research/HumanEval-Solidity", split="test")

    # 3. å¼€å§‹ç”Ÿæˆ
    results = []
    print(f"Start generating for {len(dataset)} tasks...")

    for task in tqdm(dataset):
        task_id = task['task_id']
        prompt = task['prompt'] # é¢˜ç›®æè¿°
        
        # æ„é€ è¾“å…¥
        messages = [{"role": "user", "content": prompt}]
        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer([input_text], return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.2, # æµ‹ Pass@1 å»ºè®®ä½æ¸©åº¦
                top_p=0.95,
                do_sample=True
            )
        
        # è§£ç 
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, outputs)]
        raw_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        # æ¸…æ´—
        clean_code = clean_lintseq_code(raw_output)

        # æ‹¼æ¥ï¼šPrompt (å‡½æ•°å¤´) + ç”Ÿæˆçš„ä»£ç  (å‡½æ•°ä½“)
        # æ³¨æ„ï¼šæœ‰äº›æ¨¡å‹ä¼šé‡å¤è¾“å‡º Promptï¼Œè¿™é‡Œéœ€è¦ä½ æ ¹æ®å®é™…æƒ…å†µå¾®è°ƒ
        # ç®€å•ç­–ç•¥ï¼šå¦‚æœç”Ÿæˆçš„ä¸åŒ…å« promptï¼Œå°±æ‹¼ä¸Šå»
        full_code = clean_code
        if "contract " not in clean_code and "function " not in clean_code:
             full_code = prompt + "\n" + clean_code

        results.append({
            "task_id": task_id,
            "solution": full_code, 
            "test": task['test'] # ä¿ç•™æµ‹è¯•ç”¨ä¾‹ï¼Œåé¢æµ‹ Pass@1 è¦ç”¨
        })

    # ä¿å­˜ç»“æœ
    with open(OUTPUT_FILE, "w") as f:
        json.dump(results, f, indent=2)
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œå·²ä¿å­˜åˆ° {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
